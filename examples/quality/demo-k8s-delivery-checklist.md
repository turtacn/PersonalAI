## **Kubernetes集群私有交付系统调优与规格配置检查文档**

### **前言**

本文档旨在为私有化部署的Kubernetes集群提供全面的系统调优和规格配置指导。鉴于私有环境的独特性（如无公有云弹性、对硬件和网络环境的严格控制、数据安全和SLA的极高要求），我们将聚焦于从底层硬件到操作系统、再到Kubernetes核心组件的全栈优化，以实现企业级、生产就绪的集群。参考OpenAI等大型AI/ML基础设施的经验，高并发、低延迟、高吞吐量以及极强的鲁棒性是核心追求。

**SLA核心目标：**

* **长时间稳定性：** 集群核心组件99.95%以上可用性，单点故障不影响集群功能。
* **可靠性：** 数据一致性，避免数据丢失，核心服务具备自动恢复能力。
* **数据不丢失：** Etcd数据、持久化存储卷数据、日志数据等关键数据100%不丢失。
* **性能：** API响应时间、网络吞吐、存储I/O满足应用负载需求，具备冗余性能承载突发流量。

### **通用设计原则**

1. **高可用 (HA)：**

   * 所有控制平面组件（etcd、kube-apiserver、kube-controller-manager、kube-scheduler）均采用多副本部署（推荐3或5个节点）。
   * 网络高可用：双网卡、链路聚合（LACP）、冗余交换机。
   * 电源高可用：双电源模块、冗余UPS。
   * 存储高可用：RAID、分布式存储（Ceph/GlusterFS）或高可用存储阵列。
2. **资源隔离：** 避免不同类型的工作负载相互干扰，尤其是在资源受限的环境下。
3. **可观测性：** 全面的监控（Prometheus）、日志（ELK/Loki）、告警系统，提供集群健康状态和性能指标的实时洞察。
4. **安全性：** 最小权限原则、网络策略、镜像安全扫描、运行时安全加固。
5. **备份与恢复：** etcd数据定期备份、持久化存储卷快照、集群配置备份。
6. **基线测试与压力测试：** 部署前进行硬件基线测试，集群部署后进行压力测试，验证其在预期负载下的表现。

---

### **一、基础架构层：硬件与网络配置**

| 配置项                 | 虚拟机部署场景 (VM)                                                                                                                | 裸金属部署场景 (Bare Metal)                                                                                        | 基线要求与负载规格                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 备注                                                                                                                                       |
| :------------------ | :-------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| **1. 计算资源 (CPU)**   | **CPU核数:** 根据总Pod数量和单Pod CPU需求预估，并留有20-30%的冗余。**推荐使用：** Intel Xeon Gold/Platinum 或 AMD EPYC 系列，支持虚拟化技术（VT-x/AMD-V），主频高，多核心。 | **CPU核数:** 与VM类似，但需考虑CPU缓存（L3 Cache）大小。物理核心优于超线程。**推荐使用：** Intel Xeon Gold/Platinum 或 AMD EPYC 系列，高核心数，高主频。 | **控制平面:** <br> - **etcd:** 至少4核，独立CPU核心，避免超线程影响低延迟。 <br> - **kube-apiserver:** 8核以上，根据集群规模动态调整。 <br> - **其他组件:** 2-4核。 <br> **工作节点:** <br> - **轻量级:** 8-16核 <br> - **中等/AI/ML:** 32-64核+ <br> - **GPU节点:** 至少16-32核，保证GPU有足够CPU资源配合。 <br> **基线:** CPU利用率长时间不高于70%。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | **NUMA感知:** 在多路CPU服务器上，确保操作系统和Kubernetes组件（尤其是高性能应用）能够感知并利用NUMA架构，减少内存访问延迟。VMWare vNUMA等技术有助于在VM中模拟。                                     |
| **2. 内存 (RAM)**     | **内存大小:** 根据总Pod内存需求预估，加上OS和K8s组件占用，并留有20-30%的冗余。**推荐使用：** DDR4/DDR5 ECC内存。                                                 | **内存大小:** 与VM类似，ECC内存是必须项。注意内存频率和通道数。                                                                       | **控制平面:** <br> - **etcd:** 8GB-16GB以上（根据key-value数量，推荐集群总key数量\*100MB+）。 <br> - **kube-apiserver:** 16GB-64GB（根据集群并发请求量）。 <br> - **其他组件:** 4GB-8GB。 <br> **工作节点:** <br> - **轻量级:** 32GB-64GB <br> - **中等/AI/ML:** 128GB-512GB+ <br> - **GPU节点:** 256GB-1TB+，通常GPU卡本身也带显存。 <br> **基线:** 内存利用率长时间不高于80%。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **Swap禁用:** K8s官方推荐禁用Swap，以避免I/O抖动影响性能。 `swapoff -a`。                                                                                    |
| **3. 存储 (Storage)** | **虚拟磁盘:** 配置为直通模式（Raw Device Mapping, RDM）或高性能的VMFS/NFS/iSCSI卷。使用vSphere vSAN或HCI方案时，确保底层存储性能满足要求。                          | **物理磁盘:** 直接连接的SSD/NVMe。                                                                                    | **a. 系统盘 (OS Disk):** <br> - **类型:** 固态硬盘 (SSD)。 <br> - **容量:** 100GB-200GB (根据日志、K8s组件、Docker镜像存储预留)。 <br> - **冗余:** RAID 1 (镜像)，确保系统启动盘的可用性。 <br> **b. Etcd数据盘:** <br> - **类型:** **专属、高性能NVMe SSD (强烈推荐)**。禁用共享存储。 <br> - **容量:** 至少20GB-40GB，根据集群规模和对象数量预估。 <br> - **冗余:** RAID 1 (镜像) 或 RAID 10，确保数据安全和读写性能。 <br> - **I/O性能基线:** <br>   - 随机写延迟 (p99): < 10ms (理想 < 1ms) <br>   - 顺序写吞吐: > 200 MB/s <br>   - IOPS (4KB随机写): > 5000 IOPS <br> **c. Docker/Containerd数据盘 (容器运行时):** <br> - **类型:** SSD。 <br> - **容量:** 200GB-2TB (根据镜像数量、Pod日志、容器可写层数据量)。 <br> - **冗余:** RAID 1 或 RAID 5/10。 <br> **d. 持久化存储 (PV/PVC):** <br> - **类型:** <br>   - **高性能应用 (DB, Cache, AI/ML):** NVMe SSD 或高性能企业级SSD阵列。 <br>   - **通用应用:** SATA SSD 或SAS SSD阵列。 <br>   - **大容量冷数据:** 企业级HDD阵列（如Ceph RBD/FS，NFS）。 <br> - **方案:** 选用成熟的CSI驱动（如Ceph-CSI, Rook, Longhorn, 商业存储CSI）。 <br> - **冗余:** 由底层存储系统保证（RAID, 多副本）。 | **磁盘控制器:** 硬件RAID卡 (带电池备份单元BBWC/NVRAM)，保证掉电数据一致性。 **文件系统:** XFS或Ext4，推荐XFS（尤其对于容器镜像和数据盘），其在大文件和高并发I/O场景下表现更优。挂载选项：`noatime, nodiratime`。 |
| **4. 网络 (Network)** | **虚拟网卡:** 使用高性能虚拟网卡（如VMXNET3）。保证宿主机物理网卡带宽充足。                                                                                | **物理网卡:** 至少双万兆网卡 (10GbE)，甚至25/40/100GbE。                                                                   | **a. 带宽:** <br> - **集群管理网络:** 至少千兆 (1GbE)，推荐万兆。 <br> - **Pod网络:** 至少万兆 (10GbE)，AI/ML场景推荐25/40/100GbE。 <br> - **存储网络 (如果分离):** 至少万兆 (10GbE)，推荐25/40/100GbE。 <br> **b. 冗余:** <br> - 每节点至少两张物理网卡，做链路聚合 (LACP/Bonding) 或 主备模式。 <br> - 冗余交换机设计，避免单点故障。 <br> **c. MTU:** 全链路MTU一致性（通常为1500，Overlay网络如VxLAN/Geneve可能需要调整）。 <br> **d. IP地址规划:** 为每个节点、Service CIDR、Pod CIDR、External IP、DNS等预留充足的IP地址。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | **网络延迟:** 集群内部节点间（尤其是控制平面节点）网络延迟应尽可能低，理想情况下 < 1ms。 **带宽饱和度:** 避免网络成为瓶颈，定期监控网络I/O。                                                        |
| **5. 电源管理**         | 由宿主机和数据中心层面统一管理。                                                                                                            | **双电源模块:** 服务器标配，连接到不同的UPS和市电回路。 **UPS (不间断电源):** 为所有服务器和网络设备提供足够的备用电源时间，确保系统在市电中断时能正常关机或持续运行。              | 持续供电能力，异常掉电保护。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 物理层的可靠性基石。                                                                                                                               |

---

### **二、操作系统层：Linux系统调优**

| 配置项                     | 虚拟机部署场景 (VM)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 裸金属部署场景 (Bare Metal) | 基线要求与负载规格                                                  | 备注                                                                                                     |
| :---------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------- | :--------------------------------------------------------- | :----------------------------------------------------------------------------------------------------- |
| **1. 操作系统选择**           | RHEL/CentOS 7.x/8.x, Ubuntu LTS (18.04/20.04/22.04), SUSE SLES。推荐企业级发行版，具备长期支持和更好的兼容性。                                                                                                                                                                                                                                                                                                                                                                                                                                       | 同左。                  | 稳定、安全、维护性好，具备容器友好性内核版本。                                    | 确保选择的OS版本支持所需的Kubernetes版本和容器运行时。                                                                      |
| **2. 内核参数 (sysctl)**    | **`net.bridge.bridge-nf-call-ip6tables = 1`**<br>**`net.bridge.bridge-nf-call-iptables = 1`**<br>**`net.ipv4.ip_forward = 1`**<br>**`vm.swappiness = 0`** (禁用swap)<br>**`vm.overcommit_memory = 1`** (内存过载提交)<br>**`fs.file-max = 1000000`** (最大文件句柄数)<br>**`fs.inotify.max_user_watches = 524288`**<br>**`net.ipv4.tcp_tw_reuse = 1`**<br>**`net.ipv4.tcp_fin_timeout = 30`**<br>**`net.ipv4.tcp_max_syn_backlog = 65536`**<br>**`net.core.somaxconn = 65536`**<br>**`net.netfilter.nf_conntrack_max = 1048576`** (连接跟踪表大小) | 同左。                  | 优化网络转发、文件句柄限制、内存管理、TCP参数。根据集群规模和并发连接数调整`nf_conntrack_max`。 | 配置后通过`sysctl -p`生效，并写入`/etc/sysctl.d/`或`/etc/sysctl.conf`持久化。                                          |
| **3. Cgroup 配置**        | 宿主机/VMware等虚拟化平台需配置Cgroup v1 (K8s 1.25+默认推荐Cgroup v2，但取决于操作系统版本)。                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 同左。                  | 确保Cgroup子系统完整且可被容器运行时使用。                                   | 核心，用于Kubernetes资源管理和隔离。                                                                                |
| **4. 防火墙 (Firewall)**   | 开放Kubernetes所需端口 (见下方)。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 同左。                  | 最小化开放端口，只允许必要的流量。                                          | 生产环境推荐使用iptables/nftables或firewalld配合网络策略。                                                             |
| **5. SELinux/AppArmor** | 强制模式 (Enforcing)。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 同左。                  | 保持开启并调整策略，增强系统安全性。                                         | 禁用会降低安全性。需要根据Kubernetes和容器运行时的要求进行策略调整。                                                                |
| **6. NTP 服务**           | 配置NTP客户端同步时间。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 同左。                  | 所有节点时间同步是集群正常运行的关键，避免证书过期、日志混乱等问题。                         | 使用本地NTP服务器或公共NTP服务器。                                                                                   |
| **7. 禁用不必要服务**          | 禁用与Kubernetes无关的服务，如图形界面、邮件服务、打印服务等。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 同左。                  | 减少资源消耗和攻击面。                                                | 最小化安装。                                                                                                 |
| **8. 文件系统优化**           | 对于所有数据盘 (etcd, docker, PV)，使用XFS或Ext4。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 同左。                  | `noatime, nodiratime`挂载选项，减少不必要的I/O。                       | 针对大文件和高并发I/O的优化。                                                                                       |
| **9. 磁盘I/O调度器**         | `noop`或`deadline` (针对SSD/NVMe)。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 同左。                  | 减少I/O调度延迟。                                                 | `echo noop > /sys/block/<disk>/queue/scheduler` 或 `echo deadline > /sys/block/<disk>/queue/scheduler`。 |
| **10. DNS 配置**          | `/etc/resolv.conf` 配置正确的DNS服务器，通常指向CoreDNS或Kube-DNS。                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 同左。                  | 确保集群内部和外部域名解析的正确性。                                         | 集群启动前，节点应能解析外部域名以便拉取镜像。                                                                                |
| **11. 用户文件限制**          | `ulimit -n 65536` 或更高，配置为硬限制和软限制。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 同左。                  | 增加文件句柄数，支持高并发连接。                                           | 针对`root`和`kubernetes`运行用户，在`/etc/security/limits.conf`中配置。                                             |

---

### **三、Kubernetes层：组件配置与调优**

| 配置项                              | 描述                                       | 基线要求与负载规格                                                                                                                                                                                                                                                                                                                                  | 备注                                                                                                                      |
| :------------------------------- | :--------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------- |
| **1. etcd集群**                    | **核心中的核心，其性能和稳定性直接决定集群的SLA。**            | **部署:** 3或5个独立etcd节点 (推荐奇数，确保仲裁)。<br>**硬件:** 独立NVMe SSD/SSD (见硬件部分)，独立CPU和内存。**绝不允许使用共享存储或HDD。**<br>**网络:** 专用网络（如果可能），低延迟。**确保节点间网络延迟小于1ms。**<br>**数据目录:** 独立文件系统，避免与OS/日志等混用。<br>**数据备份:** 定期备份etcd快照到远程安全存储。<br>**监控:** 监控etcd的I/O延迟、WAL同步时间、Raft心跳、leader选举耗时。**P99写操作延迟 < 10ms，WAL写延迟 < 1ms。**                                        | **日志级别:** 生产环境默认，必要时开启Debug。<br>**压缩:** 根据版本和数据量考虑开启自动compact和defragment。<br>**资源限制:** 为etcd设置严格的CPU/内存requests/limits。 |
| **2. Kube-APIServer**            | Kubernetes API的入口点，处理所有API请求。            | **部署:** 3个以上副本，通过负载均衡器 (LVS/HAProxy/Nginx/F5) 对外提供服务。<br>**硬件:** 充足的CPU和内存。<br>**端口:** 6443 (HTTPS)。<br>**参数调优:** <br>  - `--max-requests-inflight=2000` (或更高，处理并发请求) <br>  - `--min-request-timeout=300` <br>  - `--etcd-servers`: 配置etcd集群地址。<br>  - `--profiling=false` (生产环境禁用以减少开销)。<br>  - `--event-ttl`: 事件保留时间，默认1小时，大型集群可能需缩短。    | **监控:** API请求QPS、延迟、错误率、Etcd读写延迟、内存使用、Goroutine数量。                                                                      |
| **3. Kube-Controller-Manager**   | 负责维持集群期望状态。                              | **部署:** HA部署 (active/passive或active/active取决于版本和具体配置)。<br>**参数调优:** <br>  - `--concurrent-syncs`: 并发同步数量，根据集群规模调整。<br>  - `--leader-elect=true` (启用leader选举)。                                                                                                                                                                              | **监控:** Controller Manager运行状态、Leader选举情况、各项Controller同步延迟。                                                             |
| **4. Kube-Scheduler**            | 负责Pod调度。                                 | **部署:** HA部署 (active/passive)。<br>**参数调优:** <br>  - `--leader-elect=true`<br>  - `--kubeconfig`: 配置连接API Server。                                                                                                                                                                                                                           | **监控:** 调度成功率、调度延迟、待调度Pod数量。                                                                                            |
| **5. Kubelet**                   | 运行在每个工作节点上，负责Pod生命周期管理。                  | **参数调优:** <br>  - `--cpu-manager-policy=static` (针对需要CPU隔离的Pod，如AI/ML)<br>  - `--topology-manager-policy=best-effort` (CPU/GPU/NUMA感知调度)<br>  - `--system-reserved`, `--kube-reserved` (预留系统和K8s组件资源)<br>  - `--eviction-hard`: 驱逐策略 (内存/磁盘不足时)。<br>  - `--feature-gates=..."` 启用或禁用特定功能。<br>  - `--max-pods`: 单节点最大Pod数量，根据节点资源和网络插件能力调整。 | **监控:** Kubelet健康状态、Pod运行状态、资源使用率、文件系统使用率。                                                                              |
| **6. Kube-Proxy**                | 负责Service的负载均衡和网络转发。                     | **模式:** IPVS模式 (推荐，性能优于iptables，尤其在高并发Service场景)。<br>**参数调优:** <br>  - `--mode=ipvs` <br>  - `--cluster-cidr`: 集群CIDR范围。                                                                                                                                                                                                                   | **监控:** Kube-proxy运行状态、IPVS规则数量、网络连接状态。                                                                                 |
| **7. 容器运行时 (Containerd/Docker)** | 运行容器。                                    | **推荐:** Containerd (更轻量，更符合K8s CRI标准)。<br>**数据目录:** 独立SSD盘，配置`data-root`。<br>**日志驱动:** `json-file` 或 `cgroupfs`，配合日志采集系统。<br>**Cgroup驱动:** `systemd` (推荐，与kubelet保持一致)。                                                                                                                                                                    | **镜像管理:** 定期清理不再使用的镜像，避免磁盘空间耗尽。                                                                                         |
| **8. CNI 插件 (网络插件)**             | 负责Pod网络通信。                               | **推荐:** <br>  - **Calico:** 具备高性能、丰富的网络策略，适合大规模集群。 <br>  - **Cilium:** 基于eBPF，高性能、安全、可观测性强，适合高并发、低延迟场景。<br>  - **Flannel:** 简单易用，但通常不提供网络策略，适合小型或测试集群。<br>**MTU:** 确保CNI插件配置的MTU与底层网络MTU一致，避免分片。                                                                                                                                           | **网络策略:** 必须启用网络策略，限制Pod间通信，增强安全性。                                                                                      |
| **9. CSI 插件 (存储插件)**             | 负责持久化存储卷的动态供应和管理。                        | **推荐:** <br>  - **Ceph-CSI (Rook):** 适合大规模、高性能、高可用的分布式块存储和文件存储。 <br>  - **NFS CSI:** 适合简单的文件共享，但性能和HA受限于底层NFS服务器。 <br>  - **商业存储厂商CSI:** 如Dell PowerMax CSI, NetApp Trident等。 <br>  - **Local Path Provisioner:** 用于本地存储，但需要手动管理数据持久性。                                                                                                     | **数据备份与恢复:** CSI插件应支持快照和恢复功能。                                                                                           |
| **10. 资源管理 (Requests/Limits)**   | 为所有Pod配置合理的资源请求 (requests) 和限制 (limits)。 | **基线:** <br>  - **Requests:** 至少满足Pod正常运行的最低资源需求，用于调度。<br>  - **Limits:** 通常略高于Requests，防止Pod无限制地消耗资源影响其他Pod或节点稳定性。<br>  - **QoS Class:** 确保关键应用Pod获得`Guaranteed`或`Burstable` QoS。 <br>  - **CPU Limits:** 谨慎设置CPU Limits，可能导致CPU节流影响性能。优先考虑通过Requests和节点隔离来管理CPU。                                                                         | **过度分配 (Oversubscription):** 仅在明确了解应用行为和集群负载的前提下进行适度过度分配，尤其在测试或非核心业务集群。生产核心集群应尽可能避免。                                    |

---

### **四、监控、日志与告警**

1. **监控系统：**

   * **核心:** Prometheus + Grafana。
   * **Node Exporter:** 采集节点层面的CPU、内存、磁盘I/O、网络I/O等指标。
   * **Kube-State-Metrics:** 采集Kubernetes对象的指标（Pod、Deployment、Service等状态）。
   * **CAdvisor (内置于Kubelet):** 采集容器层面的资源使用。
   * **Prometheus Operator:** 简化Prometheus和Alertmanager的部署与管理。
   * **GPU 监控:** NVIDIA DCGM Exporter (用于NVIDIA GPU集群)。
   * **网络监控:** 监控网络延迟、丢包率、流量。
   * **存储监控:** 监控存储I/O性能、容量使用率、延迟。
   * **基线:** 至少保留30天监控数据，关键指标保留更长时间。

2. **日志系统：**

   * **核心:** EFK (Elasticsearch, Fluentd/Fluent Bit, Kibana) 或 PLG (Promtail, Loki, Grafana)。
   * **日志收集:** DaemonSet模式部署Agent (Fluent Bit/Promtail) 采集所有Pod、Kubelet、Docker/Containerd日志。
   * **日志存储:** 高可靠、高性能的日志存储后端 (如分布式存储、SSD阵列)。
   * **日志保留:** 根据合规性要求和故障排查需求保留至少90天甚至更长时间。

3. **告警系统：**

   * **核心:** Alertmanager。
   * **告警规则:** 定义基于Prometheus指标的告警规则，覆盖：

     * **硬件故障:** CPU/内存/磁盘利用率过高、磁盘I/O异常、网络断连。
     * **Kubernetes组件:** APIServer/Etcd/Scheduler/Controller Manager/Kubelet宕机或性能异常。
     * **节点状态:** 节点NotReady、磁盘空间不足、内存压力。
     * **Pod状态:** CrashLoopBackOff、Pending、Failed等。
     * **网络问题:** CNI插件异常、Service无法访问。
     * **存储问题:** PV无法挂载、存储I/O延迟过高。
   * **通知渠道:** 短信、邮件、钉钉、企业微信等。
   * **告警SLA:** 定义不同严重级别告警的响应时间。

---

### **五、安全与备份恢复**

1. **安全加固：**

   * **操作系统安全：** 禁用不必要的服务、定期打补丁、配置SSH密钥登录、禁用密码登录、防火墙规则。
   * **Kubernetes安全：**

     * **RBAC (Role-Based Access Control):** 最小权限原则，严格控制用户和Service Account的权限。
     * **Pod Security Admission (PSA):** 替代Pod Security Policy (PSP)，强制Pod安全标准。
     * **网络策略 (Network Policy):** 限制Pod间和Ingress/Egress流量。
     * **镜像安全:** 使用可信赖的容器镜像源，定期扫描镜像漏洞。
     * **API Server访问控制:** 限制API Server的访问IP段。
     * **Secret管理:** 使用Vault、Sealed Secrets或KMS管理敏感数据。
   * **集群准入控制器:** 启用必要的准入控制器，如`AlwaysPullImages`、`NodeRestriction`等。

2. **备份与恢复：**

   * **etcd备份:**

     * **方式:** 定期执行`etcdctl snapshot save`命令，将快照保存到安全、异地的存储。
     * **自动化:** 使用CronJob或其他自动化工具定期备份。
     * **恢复演练:** 定期进行etcd恢复演练，验证备份数据的有效性。
   * **持久化存储备份:**

     * **CSI快照:** 如果CSI驱动支持，利用CSI快照功能对PV进行备份。
     * **底层存储备份:** 利用底层存储系统（如Ceph、NAS/SAN）提供的快照和复制功能。
     * **Velero:** 开源的Kubernetes备份和迁移工具，可备份集群资源和持久卷。
   * **集群配置备份:**

     * 保存所有Kubernetes YAML文件和Helm Chart到版本控制系统 (Git)。
     * 备份Kubeconfig文件和TLS证书。

---

### **六、负载规格与容量规划示例 (基于AI/ML场景)**

以OpenAI等大型AI/ML集群为例，通常面临超大规模Pod数量、高吞吐、低延迟计算、大量GPU资源调度等挑战。

**假设：** 部署一个中型AI/ML训练和推理集群。

* **训练节点：** 20个节点，每节点8张NVIDIA A100 GPU，1TB内存，128核CPU，10TB NVMe本地存储。
* **推理节点：** 30个节点，每节点4张NVIDIA V100/A10 GPU，256GB内存，64核CPU。
* **通用计算节点：** 50个节点，每节点128GB内存，32核CPU。
* **etcd/控制平面节点：** 5个节点。

**负载规格与基线要求：**

1. **Etcd集群:**

   * **节点数:** 5个节点，独立物理机或高性能VM。
   * **CPU:** 每个节点至少8核（逻辑核），专用核心不超线程。
   * **内存:** 每个节点32GB以上。
   * **存储:** **每个节点一块专属200GB NVMe SSD，RAID 1。要求I/O延迟P99 < 5ms。**
   * **网络:** 25GbE低延迟网络。

2. **控制平面 (APIServer/Scheduler/Controller Manager):**

   * **节点数:** 5个节点，与etcd共享或独立。独立更优。
   * **CPU:** 每个节点至少16核。
   * **内存:** 每个节点64GB-128GB。
   * **网络:** 25GbE。

3. **训练/推理工作节点 (GPU节点):**

   * **硬件:**

     * **GPU:** 根据模型需求选择A100/H100/V100等。**确保GPU直通（Pass-through）到VM或裸金属直接使用。**
     * **CPU:** 匹配GPU性能，通常推荐`1-2 CPU核/GPU`。例如8张A100，则需16-32核以上。
     * **内存:** GPU显存的2-4倍。
     * **存储:** **本地NVMe SSD（2-10TB）**，用于模型缓存、中间数据、高速Checkpoints。
     * **网络:** **100GbE RDMA网络 (InfiniBand/RoCE)**，用于分布式训练通信。
   * **Kubelet配置:**

     * `--cpu-manager-policy=static`
     * `--topology-manager-policy=restricted` (确保CPU、内存、GPU在同一NUMA节点)
     * `--system-reserved`, `--kube-reserved`: 预留充足资源给系统和K8s组件。
   * **NVIDIA GPU Operator:** 必须部署，提供GPU驱动、CUDA、Container Toolkit等。
   * **监控:** 重点监控GPU利用率、温度、显存使用率、NCCL延迟等。

4. **通用计算节点:**

   * **CPU:** 32-64核。
   * **内存:** 128GB-256GB。
   * **存储:** SSD，容量根据Pod数量和临时存储需求。
   * **网络:** 25GbE。

5. **网络:**

   * **Pod网络 (数据面):** 所有节点25GbE以上，训练节点推荐100GbE InfiniBand/RoCE。
   * **集群管理网络:** 10GbE。
   * **MTU:** 全链路适配，尤其是在overlay网络和RDMA网络中。

6. **存储:**

   * **训练数据存储:** 高性能分布式文件系统 (如Lustre, OrangeFS, 或HDFS/CephFS)。
   * **模型存储/推理数据:** 高性能对象存储 (S3兼容) 或分布式块存储。
   * **持久卷:** 通过CSI对接底层高性能存储。

### **总结与持续优化**

这份文档提供了一个全面的检查清单，但请记住：

1. **没有一劳永逸的配置：** 集群的调优是一个持续的过程，随着业务负载的变化，需要不断监控、分析和调整。
2. **测试为王：** 在正式上线前，务必进行全面的压力测试、混沌工程测试，验证集群在极端情况下的稳定性。
3. **自动化：** 利用IaC (Infrastructure as Code) 工具（如Ansible, Terraform, Puppet）实现部署和配置的自动化，确保一致性和可重复性。
4. **文档化：** 详细记录所有配置、调优项、故障排除步骤和SOP。
5. **团队能力：** 确保运维团队具备处理容器、Kubernetes、Linux和底层硬件问题的能力。

